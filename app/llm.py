import requests
from app.config import MODELO, URL_OLLAMA_API

def analisar_com_ia(dados_tecnicos):
    print(f"\nüß† Sentinela: Enviando verdade t√©cnica para o {MODELO} em {URL_OLLAMA_API}...")
    
    prompt = f"""
    Voc√™ √© um Analista de SOC (Security Operations Center).
    Analise esta lista de servi√ßos REAIS rodando em um notebook de trabalho:
    
    {dados_tecnicos}
    
    Responda em PORTUGU√äS:
    1. O "AnyDesk" ou "TeamViewer" representam risco se o usu√°rio estiver em Wi-Fi p√∫blico?
    2. O "System" nas portas 135/445 √© normal?
    3. D√™ uma recomenda√ß√£o de seguran√ßa de apenas uma frase.
    """

    payload = {
        "model": MODELO,
        "prompt": prompt,
        "stream": False
    }

    try:
        resposta = requests.post(URL_OLLAMA_API, json=payload, timeout=60)
        
        if resposta.status_code != 200:
            return f"Erro na API Ollama (Status {resposta.status_code}): {resposta.text}"
            
        resposta_json = resposta.json()
        
        # Se houver erro expl√≠cito do Ollama (ex: model not found)
        if "error" in resposta_json:
             return f"Erro retornado pelo Ollama: {resposta_json['error']}"

        return resposta_json.get("response", f"IA n√£o retornou resposta v√°lida. Raw: {str(resposta_json)}")
    except requests.exceptions.ConnectionError:
        return "Erro de Conex√£o: O servidor Ollama parece estar offline ou inacess√≠vel."
    except requests.exceptions.Timeout:
        return "Timeout: O modelo demorou muito para responder (pode estar carregando)."
    except Exception as e:
        return f"Erro interno na integra√ß√£o IA: {e}"
